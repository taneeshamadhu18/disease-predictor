{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEu2kB29dUys",
        "outputId": "6e778404-8e80-421a-aede-6e218584ec85"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['model_artifacts/preprocessor_pipeline.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import joblib\n",
        "from collections import Counter\n",
        "\n",
        "df = pd.read_csv(\"iot_sensor_dataset.csv\")\n",
        "\n",
        "num_cols = [\"Water_pH\", \"Turbidity_NTU\", \"Chlorine_mg_L\",\n",
        "            \"EColi_MPN\", \"Rainfall_mm\", \"AvgTemperature_C\"]\n",
        "for c in num_cols:\n",
        "    df[c] = pd.to_numeric(df[c], errors='coerce')\n",
        "\n",
        "if 'SampleID' in df.columns:\n",
        "    df = df.drop_duplicates(subset=['SampleID'])\n",
        "\n",
        "df['pH_deviation'] = (df['Water_pH'] - 7.0).abs()\n",
        "df['chlorine_deficient'] = (df['Chlorine_mg_L'] < 0.3).astype(int)\n",
        "df['turbidity_risky'] = (df['Turbidity_NTU'] > 5.0).astype(int)\n",
        "df['turbidity_ecoli_interaction'] = df['Turbidity_NTU'] * df['EColi_MPN']\n",
        "df['Rainfall_bin'] = pd.qcut(df['Rainfall_mm'], q=3, labels=['Low', 'Medium', 'High'])\n",
        "\n",
        "X = df.drop(columns=['SampleID', 'OutbreakRisk']) if 'SampleID' in df.columns else df.drop(columns=['OutbreakRisk'])\n",
        "y = df['OutbreakRisk']\n",
        "target_map = {'Low': 0, 'Medium': 1, 'High': 2}\n",
        "y_encoded = y.map(target_map)\n",
        "if y_encoded.isnull().any():\n",
        "    raise ValueError(\"Unknown labels found in OutbreakRisk\")\n",
        "\n",
        "class IQRCapper(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, factor=1.5, columns=None):\n",
        "        self.factor = factor\n",
        "        self.columns = columns\n",
        "        self.bounds_ = {}\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        X = pd.DataFrame(X, columns=self.columns)\n",
        "        for col in self.columns:\n",
        "            q1, q3 = X[col].quantile([0.25, 0.75])\n",
        "            iqr = q3 - q1\n",
        "            self.bounds_[col] = (q1 - self.factor*iqr, q3 + self.factor*iqr)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = pd.DataFrame(X, columns=self.columns).copy()\n",
        "        for col, (lower, upper) in self.bounds_.items():\n",
        "            X[col] = X[col].clip(lower, upper)\n",
        "        return X.values\n",
        "\n",
        "def bacteria_to_binary(x):\n",
        "    return np.where(x == \"Yes\", 1, 0).reshape(-1, 1)\n",
        "\n",
        "numeric_features = [\"Water_pH\", \"Turbidity_NTU\", \"Chlorine_mg_L\",\n",
        "                    \"EColi_MPN\", \"Rainfall_mm\", \"AvgTemperature_C\",\n",
        "                    \"pH_deviation\", \"turbidity_ecoli_interaction\"]\n",
        "\n",
        "numeric_pipeline = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"iqr_capper\", IQRCapper(columns=numeric_features)),\n",
        "    (\"scaler\", StandardScaler())\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    (\"num\", numeric_pipeline, numeric_features),\n",
        "    (\"bacteria_bin\", FunctionTransformer(bacteria_to_binary), [\"BacterialPresence\"]),\n",
        "    (\"rainfall_ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), [\"Rainfall_bin\"])\n",
        "])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_encoded, test_size=0.2, stratify=y_encoded, random_state=42\n",
        ")\n",
        "\n",
        "preprocessor.fit(X_train)\n",
        "X_train_proc = preprocessor.transform(X_train)\n",
        "X_test_proc = preprocessor.transform(X_test)\n",
        "\n",
        "rainfall_ohe_categories = preprocessor.named_transformers_['rainfall_ohe'].categories_[0]\n",
        "processed_colnames = numeric_features + [\"BacterialPresence_bin\"] + [f\"Rainfall_bin_{cat}\" for cat in rainfall_ohe_categories]\n",
        "\n",
        "X_train_df = pd.DataFrame(X_train_proc, columns=processed_colnames, index=X_train.index)\n",
        "X_test_df = pd.DataFrame(X_test_proc, columns=processed_colnames, index=X_test.index)\n",
        "\n",
        "try:\n",
        "    from imblearn.over_sampling import SMOTE\n",
        "    counts = Counter(y_train)\n",
        "    if min(counts.values()) < 0.5 * max(counts.values()):\n",
        "        sm = SMOTE(random_state=42)\n",
        "        X_train_res, y_train_res = sm.fit_resample(X_train_df, y_train.values)\n",
        "        X_train_df = pd.DataFrame(X_train_res, columns=processed_colnames)\n",
        "        y_train = pd.Series(y_train_res, name='OutbreakRisk')\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "os.makedirs(\"model_artifacts\", exist_ok=True)\n",
        "joblib.dump(preprocessor, \"model_artifacts/preprocessor_pipeline.joblib\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "sk_version = sklearn.__version__\n",
        "print(sk_version)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7Jw8zrydZQ3",
        "outputId": "e0ac3f59-5bb7-4f41-be97-63e6da93fc19"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import joblib\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
        "from sklearn.pipeline import Pipeline as SKPipeline\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier, StackingClassifier\n",
        "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score, classification_report, confusion_matrix, recall_score\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "\n",
        "# Optional libs\n",
        "try:\n",
        "    from imblearn.over_sampling import SMOTE\n",
        "    from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "    IMBLEARN_AVAILABLE = True\n",
        "except ImportError:\n",
        "    IMBLEARN_AVAILABLE = False\n",
        "\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    XGBOOST_AVAILABLE = True\n",
        "except ImportError:\n",
        "    XGBOOST_AVAILABLE = False\n",
        "\n",
        "try:\n",
        "    import shap\n",
        "    SHAP_AVAILABLE = True\n",
        "except ImportError:\n",
        "    SHAP_AVAILABLE = False\n",
        "\n",
        "# Configuration\n",
        "RANDOM_STATE = 42\n",
        "N_JOBS = -1\n",
        "ARTIFACT_DIR = \"model_artifacts\"\n",
        "CSV_PATH = \"iot_sensor_dataset.csv\"\n",
        "\n",
        "# ---------- Transformers ----------\n",
        "class IQRCapper(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, factor=1.5, columns=None):\n",
        "        self.factor = factor\n",
        "        self.columns = columns\n",
        "        self.bounds_ = {}\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        Xdf = pd.DataFrame(X, columns=self.columns)\n",
        "        for col in self.columns:\n",
        "            q1, q3 = Xdf[col].quantile([0.25, 0.75])\n",
        "            iqr = q3 - q1\n",
        "            self.bounds_[col] = (q1 - self.factor*iqr, q3 + self.factor*iqr)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        Xdf = pd.DataFrame(X, columns=self.columns).copy()\n",
        "        for col, (lower, upper) in self.bounds_.items():\n",
        "            Xdf[col] = Xdf[col].clip(lower=lower, upper=upper)\n",
        "        return Xdf.values\n",
        "\n",
        "class BacteriaPresenceEncoder(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    def transform(self, X):\n",
        "        arr = np.array(X).reshape(-1,)\n",
        "        return np.where(arr == \"Yes\", 1, 0).reshape(-1, 1)\n",
        "\n",
        "# ---------- Load data ----------\n",
        "if not os.path.exists(CSV_PATH):\n",
        "    raise FileNotFoundError(f\"CSV not found at '{CSV_PATH}'.\")\n",
        "\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "\n",
        "# ---------- Preprocessing ----------\n",
        "num_cols = [\"Water_pH\", \"Turbidity_NTU\", \"Chlorine_mg_L\", \"EColi_MPN\", \"Rainfall_mm\", \"AvgTemperature_C\"]\n",
        "for c in num_cols:\n",
        "    df[c] = pd.to_numeric(df[c], errors='coerce')\n",
        "\n",
        "df['pH_deviation'] = (df['Water_pH'] - 7.0).abs()\n",
        "df['chlorine_deficient'] = (df['Chlorine_mg_L'] < 0.3).astype(int)\n",
        "df['turbidity_risky'] = (df['Turbidity_NTU'] > 5.0).astype(int)\n",
        "df['turbidity_ecoli_interaction'] = df['Turbidity_NTU'] * df['EColi_MPN']\n",
        "df['Rainfall_bin'] = pd.qcut(df['Rainfall_mm'], q=3, labels=['Low', 'Medium', 'High'])\n",
        "\n",
        "target_map = {'Low': 0, 'Medium': 1, 'High': 2}\n",
        "if 'OutbreakRisk' not in df.columns:\n",
        "    raise KeyError(\"OutbreakRisk column missing.\")\n",
        "y = df['OutbreakRisk'].map(target_map)\n",
        "if y.isnull().any():\n",
        "    raise ValueError(\"Unexpected labels in OutbreakRisk.\")\n",
        "\n",
        "X = df.drop(columns=['SampleID', 'OutbreakRisk']) if 'SampleID' in df.columns else df.drop(columns=['OutbreakRisk'])\n",
        "\n",
        "# ---------- Preprocessor ----------\n",
        "numeric_features = [\"Water_pH\", \"Turbidity_NTU\", \"Chlorine_mg_L\",\n",
        "                    \"EColi_MPN\", \"Rainfall_mm\", \"AvgTemperature_C\",\n",
        "                    \"pH_deviation\", \"turbidity_ecoli_interaction\"]\n",
        "\n",
        "numeric_pipeline = SKPipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"iqr_capper\", IQRCapper(columns=numeric_features)),\n",
        "    (\"scaler\", StandardScaler())\n",
        "])\n",
        "\n",
        "ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    (\"num\", numeric_pipeline, numeric_features),\n",
        "    (\"bacteria_bin\", BacteriaPresenceEncoder(), [\"BacterialPresence\"]),\n",
        "    (\"rainfall_ohe\", ohe, [\"Rainfall_bin\"])\n",
        "])\n",
        "\n",
        "# ---------- Train/test split ----------\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
        "                                                    stratify=y, random_state=RANDOM_STATE)\n",
        "\n",
        "# ---------- Fit preprocessor ----------\n",
        "preprocessor.fit(X_train)\n",
        "X_train_proc = preprocessor.transform(X_train)\n",
        "X_test_proc = preprocessor.transform(X_test)\n",
        "\n",
        "rainfall_categories = preprocessor.named_transformers_['rainfall_ohe'].categories_[0]\n",
        "processed_colnames = numeric_features + [\"BacterialPresence_bin\"] + [f\"Rainfall_bin_{c}\" for c in rainfall_categories]\n",
        "\n",
        "X_train_df = pd.DataFrame(X_train_proc, columns=processed_colnames, index=X_train.index)\n",
        "X_test_df = pd.DataFrame(X_test_proc, columns=processed_colnames, index=X_test.index)\n",
        "\n",
        "# ---------- Build pipeline helper ----------\n",
        "def build_pipeline(clf, use_smote=False):\n",
        "    if IMBLEARN_AVAILABLE and use_smote:\n",
        "        return ImbPipeline([(\"preproc\", preprocessor), (\"smote\", SMOTE(random_state=RANDOM_STATE)), (\"clf\", clf)])\n",
        "    else:\n",
        "        return SKPipeline([(\"preproc\", preprocessor), (\"clf\", clf)])\n",
        "\n",
        "# ---------- Candidate models ----------\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
        "models_to_search = {}\n",
        "\n",
        "lr = LogisticRegression(solver='saga', class_weight='balanced', max_iter=5000, random_state=RANDOM_STATE, n_jobs=1)\n",
        "models_to_search['logistic'] = {\n",
        "    \"pipeline\": build_pipeline(lr),\n",
        "    \"params\": {\"clf__C\": [1e-3,1e-2,1e-1,1,10,100], \"clf__penalty\": [\"l2\"]},\n",
        "    \"n_iter\": 8\n",
        "}\n",
        "\n",
        "rf = RandomForestClassifier(random_state=RANDOM_STATE, n_jobs=N_JOBS, class_weight='balanced')\n",
        "models_to_search['random_forest'] = {\n",
        "    \"pipeline\": build_pipeline(rf, use_smote=True if IMBLEARN_AVAILABLE else False),\n",
        "    \"params\": {\"clf__n_estimators\": [100,200,400], \"clf__max_depth\": [None,8,16,24],\n",
        "               \"clf__min_samples_split\":[2,5,10], \"clf__min_samples_leaf\":[1,2,4]},\n",
        "    \"n_iter\": 20\n",
        "}\n",
        "\n",
        "hgb = HistGradientBoostingClassifier(random_state=RANDOM_STATE)\n",
        "models_to_search['hist_gb'] = {\n",
        "    \"pipeline\": build_pipeline(hgb),\n",
        "    \"params\": {\"clf__learning_rate\":[0.01,0.05,0.1], \"clf__max_iter\":[100,200,400],\n",
        "               \"clf__max_depth\":[None,5,10], \"clf__l2_regularization\":[0.0,0.1,1.0]},\n",
        "    \"n_iter\": 18\n",
        "}\n",
        "\n",
        "if XGBOOST_AVAILABLE:\n",
        "    xgb_clf = xgb.XGBClassifier(objective='multi:softprob', eval_metric='mlogloss',\n",
        "                                use_label_encoder=False, n_jobs=N_JOBS, random_state=RANDOM_STATE)\n",
        "    models_to_search['xgboost'] = {\n",
        "        \"pipeline\": build_pipeline(xgb_clf),\n",
        "        \"params\": {\"clf__n_estimators\":[100,200,400], \"clf__max_depth\":[3,6,10],\n",
        "                   \"clf__learning_rate\":[0.01,0.05,0.1], \"clf__subsample\":[0.6,0.8,1.0],\n",
        "                   \"clf__colsample_bytree\":[0.6,0.8,1.0]},\n",
        "        \"n_iter\": 25\n",
        "    }\n",
        "\n",
        "# ---------- Hyperparameter tuning ----------\n",
        "results = {}\n",
        "start_time = time.time()\n",
        "for name, spec in models_to_search.items():\n",
        "    pipeline = spec[\"pipeline\"]\n",
        "    param_dist = spec[\"params\"]\n",
        "    n_iter = spec.get(\"n_iter\", 20)\n",
        "    search = RandomizedSearchCV(pipeline, param_distributions=param_dist,\n",
        "                                n_iter=n_iter, scoring='f1_macro', cv=cv,\n",
        "                                random_state=RANDOM_STATE, n_jobs=N_JOBS, verbose=0)\n",
        "    search.fit(X_train, y_train)\n",
        "    results[name] = search\n",
        "elapsed = time.time() - start_time\n",
        "\n",
        "# ---------- Evaluation helper ----------\n",
        "def evaluate_model(model_pipeline, X_test, y_test):\n",
        "    y_pred = model_pipeline.predict(X_test)\n",
        "    try:\n",
        "        y_prob = model_pipeline.predict_proba(X_test)\n",
        "    except Exception:\n",
        "        y_prob = None\n",
        "    metrics = {\n",
        "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"balanced_accuracy\": balanced_accuracy_score(y_test, y_pred),\n",
        "        \"macro_f1\": f1_score(y_test, y_pred, average='macro'),\n",
        "        \"classification_report\": classification_report(y_test, y_pred, digits=4),\n",
        "        \"confusion_matrix\": confusion_matrix(y_test, y_pred)\n",
        "    }\n",
        "    return {\"y_pred\": y_pred, \"y_prob\": y_prob, \"metrics\": metrics}\n",
        "\n",
        "# ---------- Evaluate all models ----------\n",
        "eval_summary = {name: evaluate_model(search.best_estimator_, X_test, y_test) for name, search in results.items()}\n",
        "best_name = max(eval_summary, key=lambda k: eval_summary[k][\"metrics\"][\"macro_f1\"])\n",
        "best_model_pipeline = results[best_name].best_estimator_\n",
        "\n",
        "# ---------- Calibrate ----------\n",
        "calibrator = CalibratedClassifierCV(best_model_pipeline, cv=5, method='sigmoid')\n",
        "calibrator.fit(X_train, y_train)\n",
        "\n",
        "# ---------- Save artifacts ----------\n",
        "os.makedirs(ARTIFACT_DIR, exist_ok=True)\n",
        "joblib.dump(preprocessor, os.path.join(ARTIFACT_DIR, \"preprocessor.joblib\"))\n",
        "joblib.dump(best_model_pipeline, os.path.join(ARTIFACT_DIR, f\"best_model_{best_name}.joblib\"))\n",
        "joblib.dump(calibrator, os.path.join(ARTIFACT_DIR, f\"calibrated_{best_name}.joblib\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwgqIm-heyBC",
        "outputId": "ada9b969-ea21-4d1c-a53d-ab321654f513"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_search.py:317: UserWarning: The total space of parameters 6 is smaller than n_iter=8. Running 6 iterations. For exhaustive searches, use GridSearchCV.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['model_artifacts/calibrated_hist_gb.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import joblib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class FeatureEngineer(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        # Add engineered features\n",
        "        X[\"pH_deviation\"] = (X[\"Water_pH\"] - 7.0).abs()\n",
        "        X[\"turbidity_ecoli_interaction\"] = X[\"Turbidity_NTU\"] * np.log1p(X[\"EColi_MPN\"])\n",
        "        X[\"Rainfall_bin\"] = pd.cut(\n",
        "            X[\"Rainfall_mm\"],\n",
        "            bins=[-np.inf, 20, 50, 100, np.inf],\n",
        "            labels=[\"Low\", \"Moderate\", \"High\", \"Extreme\"]\n",
        "        )\n",
        "        return X\n",
        "\n",
        "\n",
        "numeric_features = [\n",
        "    \"Water_pH\", \"Turbidity_NTU\", \"Chlorine_mg_L\",\n",
        "    \"EColi_MPN\", \"Rainfall_mm\", \"AvgTemperature_C\",\n",
        "    \"pH_deviation\", \"turbidity_ecoli_interaction\"\n",
        "]\n",
        "categorical_features = [\"BacterialPresence\", \"Rainfall_bin\"]\n",
        "\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", StandardScaler(), numeric_features),\n",
        "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features)\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "final_pipeline = Pipeline(steps=[\n",
        "    (\"features\", FeatureEngineer()),\n",
        "    (\"preprocess\", preprocessor),\n",
        "    (\"model\", HistGradientBoostingClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "X = df.drop(columns=[\"OutbreakRisk\"])\n",
        "y = df[\"OutbreakRisk\"]\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "final_pipeline.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "y_pred = final_pipeline.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "\n",
        "joblib.dump(final_pipeline, \"model_artifacts/final_pipeline.joblib\")\n",
        "print(\"✅ Final pipeline saved as model_artifacts/final_pipeline.joblib\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSkoxWrE-Fw4",
        "outputId": "2ed82cfa-0ed7-4010-dd74-f63567f6839d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9975\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        High       1.00      0.99      1.00       152\n",
            "         Low       1.00      1.00      1.00        49\n",
            "      Medium       0.99      1.00      1.00       199\n",
            "\n",
            "    accuracy                           1.00       400\n",
            "   macro avg       1.00      1.00      1.00       400\n",
            "weighted avg       1.00      1.00      1.00       400\n",
            "\n",
            "✅ Final pipeline saved as model_artifacts/final_pipeline.joblib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import pandas as pd\n",
        "\n",
        "pipeline = joblib.load(\"model_artifacts/final_pipeline.joblib\")\n",
        "\n",
        "sample = pd.DataFrame([{\n",
        "    \"Water_pH\": 6.5,\n",
        "    \"Turbidity_NTU\": 4.0,\n",
        "    \"Chlorine_mg_L\": 0.3,\n",
        "    \"EColi_MPN\": 40,\n",
        "    \"Rainfall_mm\": 55.0,\n",
        "    \"AvgTemperature_C\": 30,\n",
        "    \"BacterialPresence\": \"No\"\n",
        "}])\n",
        "\n",
        "\n",
        "pred_class = pipeline.predict(sample)[0]\n",
        "pred_prob = pipeline.predict_proba(sample)[0]\n",
        "\n",
        "print(\"Predicted class:\", pred_class)\n",
        "print(\"Probabilities (Low, Medium, High):\", pred_prob)\n"
      ],
      "metadata": {
        "id": "8aq2su5cyWDO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfaf7101-2e33-4588-da83-ed45b2d8b345"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class: Medium\n",
            "Probabilities (Low, Medium, High): [1.29775353e-07 2.28451944e-08 9.99999847e-01]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uK3Cei9Z86NA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}